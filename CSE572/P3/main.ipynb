{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c1af28d-2341-4a3e-b969-00ce3ec4b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Meal Features (first row): [-0.39491061 -1.04644436 -0.41067801  1.40382966 -0.64378146]\n",
      "Aligned Ground Truth Labels (first 10 labels): [1 0 0 3 0 0 1 0 1 0]\n",
      "NaNs in feature matrix: 0\n",
      "KMeans labels: [0 1]\n",
      "DBSCAN labels: [-1  0]\n",
      "KMeans SSE: 2086.832743031122\n",
      "DBSCAN SSE: 1816.4197345635012\n",
      "DBSCAN Silhouette Score: 0.4419341610011935\n",
      "kmeans entropy: 0.6117155457018411\n",
      "DBSCAN entropy: 0.20121056126939535\n",
      "kmeans purity: 0.7585513078470825\n",
      "DBSCAN purity: 0.9356136820925554\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import fft\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def find_optimal_eps(X):\n",
    "    neighbors = NearestNeighbors(n_neighbors=5)\n",
    "    neighbors_fit = neighbors.fit(X)\n",
    "    distances, indices = neighbors_fit.kneighbors(X)\n",
    "    distances = np.sort(distances[:, 4], axis=0)\n",
    "    return distances\n",
    "\n",
    "def plot_distances(distances):\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel(\"Data Points\")\n",
    "    plt.ylabel(\"Distance to 5th Nearest Neighbor\")\n",
    "    plt.title(\"K-Distance Graph to find optimal eps\")\n",
    "    plt.show()\n",
    "\n",
    "def apply_pca(X, n_components=5):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca\n",
    "\n",
    "def scale_features(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled\n",
    "\n",
    "def feature_extractor(data_matrix, time_interval=5):\n",
    "    data_matrix_df = pd.DataFrame(data_matrix)\n",
    "    feature_matrix = pd.DataFrame()\n",
    "    \n",
    "    feature_matrix['mean'] = data_matrix_df.mean(axis=1)\n",
    "    feature_matrix['std'] = data_matrix_df.std(axis=1)\n",
    "    feature_matrix['rate_of_change'] = data_matrix_df.diff(axis=1).mean(axis=1)\n",
    "    feature_matrix['time_to_peak'] = data_matrix_df.idxmax(axis=1) * time_interval\n",
    "    feature_matrix['auc'] = np.trapz(data_matrix, axis=1)  # NumPy method, no need to convert\n",
    "    \n",
    "    feature_matrix['fft_energy'] = np.abs(np.fft.fft(data_matrix, axis=1)).mean(axis=1)  # NumPy method, no need to convert\n",
    "    feature_matrix['cv_glucose'] = data_matrix_df.std(axis=1) / data_matrix_df.mean(axis=1)\n",
    "    feature_matrix['peak_amplitude'] = data_matrix_df.max(axis=1) - data_matrix_df.min(axis=1)\n",
    "    feature_matrix['time_above_180'] = (data_matrix_df > 180).mean(axis=1)\n",
    "    feature_matrix['skewness'] = data_matrix_df.apply(lambda x: skew(x), axis=1)\n",
    "    feature_matrix['kurtosis'] = data_matrix_df.apply(lambda x: kurtosis(x), axis=1)\n",
    "    feature_matrix['energy'] = (data_matrix ** 2).sum(axis=1)\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "def generate_meal_feature_matrix(insulin_data, cgm_data):\n",
    "    meal_times = insulin_data[insulin_data['BWZ Carb Input (grams)'].notna() & (insulin_data['BWZ Carb Input (grams)'] > 0)]\n",
    "    feature_matrix = pd.DataFrame()\n",
    "\n",
    "    for meal_time in meal_times['Timestamp']:\n",
    "        cgm_meal_data = cgm_data[(cgm_data['Timestamp'] >= meal_time - pd.Timedelta(hours=2)) & (cgm_data['Timestamp'] <= meal_time + pd.Timedelta(hours=2.5))]\n",
    "\n",
    "        if len(cgm_meal_data) >= 24:\n",
    "            glucose_values = cgm_meal_data[['Sensor Glucose (mg/dL)']].values.T\n",
    "            meal_features = feature_extractor(glucose_values)\n",
    "            feature_matrix = pd.concat([feature_matrix, meal_features], ignore_index=True)\n",
    "\n",
    "    return feature_matrix\n",
    "\n",
    "def process_and_extract_features(insulin_data, cgm_data):\n",
    "    insulin_data['Timestamp'] = pd.to_datetime(insulin_data['Date'] + ' ' + insulin_data['Time'])\n",
    "    cgm_data['Timestamp'] = pd.to_datetime(cgm_data['Date'] + ' ' + cgm_data['Time'])\n",
    "\n",
    "    meal_feature_matrix = generate_meal_feature_matrix(insulin_data, cgm_data)\n",
    "    return meal_feature_matrix\n",
    "\n",
    "def extract_ground_truth(insulin_data):\n",
    "    carb_input = insulin_data['BWZ Carb Input (grams)'].dropna()\n",
    "    min_carb = carb_input.min()\n",
    "    max_carb = carb_input.max()\n",
    "\n",
    "    bins = np.arange(min_carb, max_carb + 20, 20)\n",
    "    \n",
    "    ground_truth_labels = pd.cut(carb_input, bins, labels=False, include_lowest=True)\n",
    "    \n",
    "    return carb_input.values, ground_truth_labels.values\n",
    "\n",
    "def cluster_data(n_bins, meal_feature_matrix):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    meal_feature_matrix_imputed = imputer.fit_transform(meal_feature_matrix)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    meal_feature_matrix_scaled = scaler.fit_transform(meal_feature_matrix_imputed)\n",
    "    kmeans = KMeans(n_clusters=n_bins, init='k-means++', n_init=20, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(meal_feature_matrix_scaled)\n",
    "\n",
    "    #optimal_eps = find_optimal_eps(meal_feature_matrix_scaled)\n",
    "    #plot_distances(optimal_eps)\n",
    "    dbscan = DBSCAN(eps=1.5, min_samples=10)\n",
    "    dbscan_labels = dbscan.fit_predict(meal_feature_matrix_scaled)\n",
    "\n",
    "    return kmeans, kmeans_labels, dbscan_labels\n",
    "\n",
    "def calculate_sse_for_kmeans(data, labels, cluster_centers):\n",
    "    sse = 0\n",
    "    for i, center in enumerate(cluster_centers):\n",
    "        cluster_points = data[labels == i]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            sse += np.sum((cluster_points - center) ** 2)\n",
    "    \n",
    "    return sse\n",
    "\n",
    "def calculate_sse_for_dbscan(data, labels):\n",
    "    sse = 0\n",
    "    for label in np.unique(labels):\n",
    "        if label != -1:  # Skip noise points\n",
    "            cluster_points = data[labels == label]\n",
    "            cluster_center = cluster_points.mean(axis=0)\n",
    "            sse += np.sum((cluster_points - cluster_center) ** 2)\n",
    "    \n",
    "    return sse\n",
    "\n",
    "def calculate_entropy(cluster_labels, ground_truth_labels, n_clusters):\n",
    "    entropy = 0\n",
    "    contingency_matrix = confusion_matrix(ground_truth_labels, cluster_labels)\n",
    "    for i in range(n_clusters):\n",
    "        cluster_size = np.sum(contingency_matrix[i])\n",
    "        if cluster_size == 0:\n",
    "            continue\n",
    "        cluster_entropy = 0\n",
    "        for j in range(len(contingency_matrix[i])):\n",
    "            if contingency_matrix[i][j] > 0:\n",
    "                probability = contingency_matrix[i][j] / cluster_size\n",
    "                cluster_entropy -= probability * np.log2(probability)\n",
    "        entropy += (cluster_size / len(ground_truth_labels)) * cluster_entropy\n",
    "    return entropy\n",
    "\n",
    "def calculate_purity(cluster_labels, ground_truth_labels, n_clusters):\n",
    "    contingency_matrix = confusion_matrix(ground_truth_labels, cluster_labels)\n",
    "    purity = np.sum(np.amax(contingency_matrix, axis=1)) / len(ground_truth_labels)\n",
    "    return purity\n",
    "\n",
    "def write_results_to_csv(sse_kmeans, sse_dbscan, entropy_kmeans, entropy_dbscan, purity_kmeans, purity_dbscan, output_file):\n",
    "    results = [sse_kmeans, sse_dbscan, entropy_kmeans, entropy_dbscan, purity_kmeans, purity_dbscan]\n",
    "    pd.DataFrame([results]).to_csv(output_file, header=None, index=False)\n",
    "\n",
    "def align_data(meal_feature_matrix, ground_truth_labels):\n",
    "    # Ensure both are numpy arrays\n",
    "    meal_feature_matrix = np.asarray(meal_feature_matrix)\n",
    "    ground_truth_labels = np.asarray(ground_truth_labels)\n",
    "    \n",
    "    # Ensure both have the same length\n",
    "    min_len = min(len(meal_feature_matrix), len(ground_truth_labels))\n",
    "    \n",
    "    # Align both datasets\n",
    "    aligned_meal_features = meal_feature_matrix[:min_len]\n",
    "    aligned_ground_truth = ground_truth_labels[:min_len]\n",
    "    \n",
    "    # Debugging: Print sample data to inspect\n",
    "    print(f\"Aligned Meal Features (first row): {aligned_meal_features[0]}\")\n",
    "    print(f\"Aligned Ground Truth Labels (first 10 labels): {aligned_ground_truth[:10]}\")\n",
    "    \n",
    "    return aligned_meal_features, aligned_ground_truth\n",
    "\n",
    "def main():\n",
    "    insulin_file = './InsulinData.csv'\n",
    "    cgm_file = './CGMData.csv'\n",
    "    insulin_data = pd.read_csv(insulin_file, low_memory=False)\n",
    "    cgm_data = pd.read_csv(cgm_file, low_memory=False)\n",
    "    \n",
    "    meal_feature_matrix_dirty = process_and_extract_features(insulin_data, cgm_data)\n",
    "    meal_feature_matrix_clean = meal_feature_matrix_dirty.dropna()\n",
    "    meal_feature_matrix_pca = apply_pca(meal_feature_matrix_clean, n_components=5)\n",
    "    meal_feature_matrix_scaled = scale_features(meal_feature_matrix_pca)\n",
    "    # Step 1: Extract ground truth labels\n",
    "    carb_input, ground_truth_labels = extract_ground_truth(insulin_data)\n",
    "    meal_feature_matrix, ground_truth_labels = align_data(meal_feature_matrix_scaled, ground_truth_labels)\n",
    "    print(f\"NaNs in feature matrix: {np.isnan(meal_feature_matrix).sum()}\")\n",
    "    \n",
    "    # Step 2: Perform clustering (load meal feature matrix)\n",
    "    n_bins = 2\n",
    "    kmeans, kmeans_labels, dbscan_labels = cluster_data(n_bins, meal_feature_matrix)\n",
    "    print(f\"KMeans labels: {np.unique(kmeans_labels)}\")\n",
    "    print(f\"DBSCAN labels: {np.unique(dbscan_labels)}\")\n",
    "    \n",
    "    # Step 3: Calculate SSE\n",
    "    kmeans_sse = calculate_sse_for_kmeans(meal_feature_matrix, kmeans_labels, kmeans.cluster_centers_)\n",
    "    dbscan_sse = calculate_sse_for_dbscan(meal_feature_matrix, dbscan_labels) \n",
    "    print(f\"KMeans SSE: {kmeans_sse}\")\n",
    "    print(f\"DBSCAN SSE: {dbscan_sse}\")\n",
    "    dbscan_silhouette = silhouette_score(meal_feature_matrix, dbscan_labels)\n",
    "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette}\")\n",
    "    # Step 4: Calculate entropy and purity\n",
    "    kmeans_entropy = calculate_entropy(kmeans_labels, ground_truth_labels, n_bins)\n",
    "    dbscan_entropy = calculate_entropy(dbscan_labels, ground_truth_labels, n_bins)\n",
    "    print(f\"kmeans entropy: {kmeans_entropy}\")\n",
    "    print(f\"DBSCAN entropy: {dbscan_entropy}\")\n",
    "    kmeans_purity = calculate_purity(kmeans_labels, ground_truth_labels, n_bins)\n",
    "    dbscan_purity = calculate_purity(dbscan_labels, ground_truth_labels, n_bins)\n",
    "    print(f\"kmeans purity: {kmeans_purity}\")\n",
    "    print(f\"DBSCAN purity: {dbscan_purity}\")\n",
    "    # Step 5: Output results to CSV\n",
    "    write_results_to_csv(kmeans_sse, dbscan_sse, kmeans_entropy, dbscan_entropy, kmeans_purity, dbscan_purity, 'Result.csv')\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a9795-2322-454b-9f43-60e4862bbec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
