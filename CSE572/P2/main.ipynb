{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8e9c1a0-a042-4d1b-ad30-ff168f5578b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Data saved to meal_data_knn.pkl\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Insufficient data for KNN imputation, skipping...\n",
      "Data saved to no_meal_data_knn.pkl\n",
      "Saved meal and no-meal data for method: knn\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import pickle\n",
    "\n",
    "def handle_missing_data(data, method='ignore', threshold=0.1):\n",
    "    \"\"\"Handle missing data in a given time series stretch.\n",
    "    Arguments:\n",
    "    - data: pandas DataFrame with glucose values.\n",
    "    - method: strategy for handling missing data ('ignore', 'linear', 'polynomial', 'knn').\n",
    "    - threshold: fraction of missing data points to consider a stretch invalid.\n",
    "    \n",
    "    Returns:\n",
    "    - data: cleaned data based on the selected method.\n",
    "    \"\"\"\n",
    "    missing_fraction = data.isna().mean().mean()\n",
    "    if missing_fraction > threshold:\n",
    "        return None\n",
    "\n",
    "    if method == 'linear':\n",
    "        return data.interpolate(method='linear')\n",
    "\n",
    "    if method == 'polynomial':\n",
    "        return data.interpolate(method='polynomial', order=2)\n",
    "\n",
    "    if method == 'knn':\n",
    "        if len(data.dropna()) > 0:\n",
    "            imputer = KNNImputer(n_neighbors=3)\n",
    "            return pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "        else:\n",
    "            # \"Insufficient data for KNN imputation, skipping...\"\n",
    "            return None\n",
    "    return data.dropna()\n",
    "\n",
    "def extract_base_data():\n",
    "    insulin_file = './InsulinData.csv'\n",
    "    cgm_file = './CGMData.csv'\n",
    "    insulin_file_two = './Insulin_patient2.csv'\n",
    "    cgm_file_two = './CGM_patient2.csv'\n",
    "    insulin_data_one = pd.read_csv(insulin_file, low_memory=False)\n",
    "    cgm_data_one = pd.read_csv(cgm_file, low_memory=False)\n",
    "    insulin_data_two = pd.read_csv(insulin_file_two, low_memory=False)\n",
    "    cgm_data_two = pd.read_csv(cgm_file_two, low_memory=False)\n",
    "\n",
    "    insulin_data = pd.concat([insulin_data_one, insulin_data_two], ignore_index=True)\n",
    "    cgm_data = pd.concat([cgm_data_one, cgm_data_two], ignore_index=True)\n",
    "    \n",
    "    cgm_data['Timestamp'] = pd.to_datetime(cgm_data['Date'] + ' ' + cgm_data['Time'])\n",
    "    insulin_data['Timestamp'] = pd.to_datetime(insulin_data['Date'] + ' ' + insulin_data['Time'])\n",
    "    return insulin_data, cgm_data \n",
    "\n",
    "def extract_meal_data(insulin_data, cgm_data, missing_data_method='ignore', threshold=0.1):\n",
    "    # COLUMNS\n",
    "    TIME = 'Timestamp'\n",
    "    CARB_INPUT = 'BWZ Carb Input (grams)'\n",
    "    GLUCOSE = 'Sensor Glucose (mg/dL)'\n",
    "    \n",
    "    meal_times = insulin_data[insulin_data[CARB_INPUT].notna() & (insulin_data[CARB_INPUT] > 0)]\n",
    "\n",
    "    meal_data_matrix = []\n",
    "    for _, meal in meal_times.iterrows():\n",
    "        tm = meal[TIME]\n",
    "        start_time = tm - pd.Timedelta(minutes=30)\n",
    "        end_time = tm + pd.Timedelta(hours=2)\n",
    "        cgm_time_series = cgm_data[(cgm_data[TIME] >= start_time) & (cgm_data[TIME] <= end_time)]\n",
    "\n",
    "        # Check for conditions described:\n",
    "        # 4a. No other meal in the tm to tm + 2 hours period\n",
    "        later_meals = meal_times[(meal_times[TIME] > tm) & (meal_times[TIME] < end_time)]\n",
    "        \n",
    "        if later_meals.empty:\n",
    "            # Condition 4a: No meal within the next 2 hours, use this stretch as meal data\n",
    "            meal_data = cgm_time_series\n",
    "        else:\n",
    "            # 4b: If there is a meal in between tm and tm + 2 hours, consider that instead\n",
    "            tp = later_meals.iloc[0][TIME]\n",
    "            cgm_time_series = cgm_data[(cgm_data[TIME] >= start_time) & (cgm_data[TIME] <= end_time)]\n",
    "            meal_data = cgm_time_series\n",
    "        \n",
    "        # 4c: If there is a meal exactly at tm + 2 hours, adjust the time window\n",
    "        if not later_meals.empty and later_meals.iloc[0][TIME] == end_time:\n",
    "            cgm_time_series = cgm_data[(cgm_data[TIME] >= tm + pd.Timedelta(hours=1.5)) & (cgm_data[TIME] <= tm + pd.Timedelta(hours=4))]\n",
    "            meal_data = cgm_time_series\n",
    "        \n",
    "        meal_data_cleaned = handle_missing_data(meal_data[[GLUCOSE]], method=missing_data_method, threshold=threshold)\n",
    "\n",
    "        if meal_data_cleaned is not None:\n",
    "            # Ensure each meal stretch has 30 columns (2hr 30 min of data at 5-min intervals)\n",
    "            if len(meal_data_cleaned) >= 30:\n",
    "                meal_data_cleaned = meal_data_cleaned.iloc[:30]  # Truncate to 30 rows if there are more than 30\n",
    "                meal_data_matrix.append(meal_data_cleaned.values.flatten())\n",
    "\n",
    "    meal_data_matrix = pd.DataFrame(meal_data_matrix)\n",
    "    return meal_data_matrix\n",
    "\n",
    "\n",
    "def extract_no_meal_data(insulin_data, cgm_data, missing_data_method='ignore', threshold=0.1):\n",
    "    # COLUMNS\n",
    "    TIME = 'Timestamp'\n",
    "    CARB_INPUT = 'BWZ Carb Input (grams)'\n",
    "    GLUCOSE = 'Sensor Glucose (mg/dL)'\n",
    "    \n",
    "    meal_times = insulin_data[insulin_data[CARB_INPUT].notna() & (insulin_data[CARB_INPUT] > 0)]\n",
    "    no_meal_data_matrix = []\n",
    "\n",
    "    for _, meal in meal_times.iterrows():\n",
    "        tm = meal[TIME] \n",
    "        post_absorptive_start = tm + pd.Timedelta(hours=2) \n",
    "        post_absorptive_end = post_absorptive_start + pd.Timedelta(hours=2)\n",
    "        post_absorptive_meals = insulin_data[(insulin_data[CARB_INPUT].notna()) &\n",
    "                                             (insulin_data[CARB_INPUT] > 0) &\n",
    "                                             (insulin_data[TIME] > post_absorptive_start) &\n",
    "                                             (insulin_data[TIME] <= post_absorptive_end)]\n",
    "        \n",
    "        if post_absorptive_meals.empty:\n",
    "            no_meal_data = cgm_data[(cgm_data[TIME] >= post_absorptive_start) & (cgm_data[TIME] < post_absorptive_end)]\n",
    "            no_meal_data_cleaned = handle_missing_data(no_meal_data[[GLUCOSE]], method=missing_data_method, threshold=threshold)\n",
    "    \n",
    "            if no_meal_data_cleaned is not None and len(no_meal_data_cleaned) >= 24:\n",
    "                no_meal_data_cleaned = no_meal_data_cleaned.iloc[:24]\n",
    "                no_meal_data_matrix.append(no_meal_data_cleaned.values.flatten())\n",
    "    \n",
    "        # Condition 2b: If a meal found in post-absorptive period has 0 carbs, ignore this meal stretch and use no meal data\n",
    "        elif post_absorptive_meals.iloc[0][CARB_INPUT] == 0:\n",
    "            no_meal_data = cgm_data[(cgm_data[TIME] >= post_absorptive_start) & (cgm_data[TIME] < post_absorptive_end)]\n",
    "            no_meal_data_cleaned = handle_missing_data(no_meal_data[[GLUCOSE]], method=missing_data_method, threshold=threshold)\n",
    "    \n",
    "            if no_meal_data_cleaned is not None and len(no_meal_data_cleaned) >= 24:\n",
    "                no_meal_data_cleaned = no_meal_data_cleaned.iloc[:24] \n",
    "                no_meal_data_matrix.append(no_meal_data_cleaned.values.flatten())\n",
    "\n",
    "    no_meal_data_matrix = pd.DataFrame(no_meal_data_matrix)\n",
    "    return no_meal_data_matrix\n",
    "\n",
    "def save_data_to_pickle(data, filename):\n",
    "    \"\"\"Save data to a pickle file.\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def load_data_from_pickle(filename):\n",
    "    \"\"\"Load data from a pickle file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"Data loaded from {filename}\")\n",
    "    return data\n",
    "\n",
    "def save_method(method):\n",
    "    insulin_data, cgm_data = extract_base_data()\n",
    "    insulin_data = insulin_data.copy()\n",
    "    cgm_data = cgm_data.copy()\n",
    "    \n",
    "    meal_data_matrix = extract_meal_data(insulin_data, cgm_data, missing_data_method=method)\n",
    "    meal_filename = f\"meal_data_{method}.pkl\"\n",
    "    save_data_to_pickle(meal_data_matrix, meal_filename)\n",
    "    \n",
    "    no_meal_data_matrix = extract_no_meal_data(insulin_data, cgm_data, missing_data_method=method)\n",
    "    no_meal_filename = f\"no_meal_data_{method}.pkl\"\n",
    "    save_data_to_pickle(no_meal_data_matrix, no_meal_filename)\n",
    "    \n",
    "    print(f\"Saved meal and no-meal data for method: {method}\")\n",
    "\n",
    "def iterate_and_save_methods(method='all'):\n",
    "    \"\"\"\n",
    "    Iterate over missing data methods, extract meal and no-meal data,\n",
    "    and save to pickle files for later use.\n",
    "    \"\"\"\n",
    "    if method == 'all':\n",
    "        for method in ['knn', 'linear', 'polynomial', 'ignore']:\n",
    "            save_method(method)\n",
    "    else:\n",
    "        save_method(method)\n",
    "\n",
    "iterate_and_save_methods('knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f350fb35-773f-4451-9bd6-c6c1a1555469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import fft\n",
    "\n",
    "def load_data_matrices(method):\n",
    "    meal_filename = f\"meal_data_{method}.pkl\"\n",
    "    with open(meal_filename, 'rb') as file:\n",
    "        meal_data_matrix = pickle.load(file)\n",
    "\n",
    "    no_meal_filename = f\"no_meal_data_{method}.pkl\"\n",
    "    with open(no_meal_filename, 'rb') as file:\n",
    "        no_meal_data_matrix = pickle.load(file)\n",
    "\n",
    "    meal_data_df = pd.DataFrame(meal_data_matrix)\n",
    "    no_meal_data_df = pd.DataFrame(no_meal_data_matrix)\n",
    "\n",
    "    return meal_data_df, no_meal_data_df\n",
    "\n",
    "def feature_extractor(data_matrix, time_interval=5):\n",
    "    feature_matrix = pd.DataFrame()\n",
    "    \n",
    "    feature_matrix['mean'] = data_matrix.mean(axis=1)\n",
    "    feature_matrix['std'] = data_matrix.std(axis=1)\n",
    "    feature_matrix['rate_of_change'] = data_matrix.diff(axis=1).mean(axis=1)\n",
    "    feature_matrix['time_to_peak'] = data_matrix.idxmax(axis=1) * time_interval\n",
    "    feature_matrix['auc'] = np.trapz(data_matrix, axis=1)\n",
    "    feature_matrix['fft_energy'] = np.abs(np.fft.fft(data_matrix, axis=1)).mean(axis=1)\n",
    "    feature_matrix['cv_glucose'] = data_matrix.std(axis=1) / data_matrix.mean(axis=1)\n",
    "    feature_matrix['peak_amplitude'] = data_matrix.max(axis=1) - data_matrix.min(axis=1)\n",
    "    feature_matrix['time_above_180'] = (data_matrix > 180).mean(axis=1)\n",
    "    feature_matrix['skewness'] = data_matrix.apply(lambda x: skew(x, axis=0), axis=1)\n",
    "    feature_matrix['kurtosis'] = data_matrix.apply(lambda x: kurtosis(x, axis=0), axis=1)\n",
    "    feature_matrix['energy'] = (data_matrix ** 2).sum(axis=1)\n",
    "    return feature_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bc83381-d8f1-4b93-80ec-8fb56a36fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing knn\n",
      "Best C: 0.01\n",
      "Best cross-validation score: 0.9258431141127359\n",
      "Test set accuracy with best C: 0.9406\n",
      "######## ending knn\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_machine_with_kfold(meal_feature_matrix, no_meal_feature_matrix, missing_data_method='mean', k=5):\n",
    "    X = np.vstack([meal_feature_matrix, no_meal_feature_matrix])\n",
    "    y = np.hstack([np.ones(len(meal_feature_matrix)), np.zeros(len(no_meal_feature_matrix))])\n",
    "\n",
    "    if missing_data_method == 'mean':\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "    elif missing_data_method == 'median':\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "    elif missing_data_method == 'most_frequent':\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "    elif missing_data_method == 'remove':\n",
    "        X_imputed = X[~np.isnan(X).any(axis=1)]\n",
    "        y = y[~np.isnan(X).any(axis=1)] \n",
    "    else:\n",
    "        raise ValueError(\"Invalid missing data method. Choose from 'mean', 'median', 'most_frequent', or 'remove'.\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled , y, test_size=0.2, random_state=42)\n",
    "    param_grid = {'C': [0.001,0.005,0.009,0.01]}\n",
    "    \n",
    "    model = SVC(kernel='linear', class_weight='balanced')\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=k, scoring='accuracy')\n",
    "    grid_search.fit(X_scaled, y)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_C = grid_search.best_params_['C']\n",
    "    \n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    final_accuracy = best_model.score(X_test, y_test)\n",
    "    print(f\"Test set accuracy with best C: {final_accuracy:.4f}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_with_missing_method(method, missing_data_method):\n",
    "    print(f\"processing {method}\")\n",
    "    meal_data_df, no_meal_data_df = load_data_matrices(method)\n",
    "    meal_features = feature_extractor(meal_data_df)\n",
    "    no_meal_features = feature_extractor(no_meal_data_df)\n",
    "    model = train_machine_with_kfold(meal_features, no_meal_features, missing_data_method='remove')\n",
    "    with open(f'trained_model_{method}.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f\"######## ending {method}\")\n",
    "\n",
    "#missing_data_methods = ['ignore', 'linear', 'knn', 'polynomial']\n",
    "#missing_data_method_two = ['remove', 'mean', 'median', 'most_frequent']\n",
    "train_with_missing_method('knn', missing_data_method='remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d8125eb-4cfe-403e-ad71-849e803f2df8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_extractor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_test_csv\u001b[39m(test_csv_path, model_pickle_file, result_csv_path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_pickle_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from train import feature_extractor\n",
    "\n",
    "def process_test_csv(test_csv_path, model_pickle_file, result_csv_path):\n",
    "    with open(model_pickle_file, \"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    test_data_frame = pd.read_csv(test_csv_path, header=None)\n",
    "    \n",
    "    test_features = feature_extractor(test_data_frame)\n",
    "    \n",
    "    pd.DataFrame(model.predict(test_features)).to_csv(result_csv_path, header=None, index=False)\n",
    "\n",
    "\n",
    "process_test_csv('./test.csv', './trained_model_knn.pkl', 'Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e52ea3-9e21-43ea-91d2-6b6f80dcaf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
