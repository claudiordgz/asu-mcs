{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fead8e2-5957-49c7-aec1-2dc22b2d306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 59\n"
     ]
    }
   ],
   "source": [
    "a2 = [2, 2, 4, 3, 2, 0, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 2,]\n",
    "b2 = [3, 3, 4, 4, 2, 2, 3, 4, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3,]\n",
    "print(sum(a2), sum(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5888793c-a1a2-4c9b-9920-acba5023283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 62\n"
     ]
    }
   ],
   "source": [
    "a = [3, 2, 4, 3, 2, 4, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 2,]\n",
    "b = [4, 3, 4, 4, 2, 2, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3,]\n",
    "print(sum(a), sum(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8a53a8-9194-48df-a23e-d2a0bc419615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 60\n"
     ]
    }
   ],
   "source": [
    "a3 = [4, 2, 4, 4, 2, 4, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 3,]\n",
    "b3 = [3, 3, 4, 4, 2, 1, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3,]\n",
    "print(sum(a3), sum(b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5b04e31-78ab-4f09-b2f7-7b2a2f2ac83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 66\n"
     ]
    }
   ],
   "source": [
    "a4 = [4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4, 3, 3, 4, 4,]\n",
    "b4 = [4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4, 3, 3, 4, 4,]\n",
    "print(sum(a4), sum(b4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f10c7b12-48da-4ff6-8fff-b0c83c2a0767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 68\n"
     ]
    }
   ],
   "source": [
    "a5 = [4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 4, 4,]\n",
    "b5 = [4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 4, 4,]\n",
    "print(sum(a5), sum(b5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c851a647-f72a-43f2-9f41-df25012ef853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for Attribute 1: 0.22943684069673975\n",
      "Information Gain for Attribute 2: 0.007214618474517431\n",
      "Information Gain for Attribute 3 (split at median): 0.007214618474517431\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "'''\n",
    "Review the table labeled Figure 1: Training Dataset. Attribute 3 is continuous. Assume that we want to use a decision tree for modeling the data.\n",
    "\n",
    "Using entropy as the measure of node impurity to calculate information gain, which attribute will provide the best split?\n",
    "'''\n",
    "\n",
    "# Create the data\n",
    "data = {\n",
    "    'Attribute1': ['T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F'],\n",
    "    'Attribute2': ['T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T'],\n",
    "    'Attribute3': [1, 6, 5, 4, 7, 3, 8, 7, 5],\n",
    "    'Class': ['Y', 'Y', 'N', 'Y', 'N', 'N', 'N', 'Y', 'N']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def entropy(y):\n",
    "    class_labels = y.unique()\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        p_label = len(y[y == label]) / len(y)\n",
    "        entropy -= p_label * np.log2(p_label)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(df, split_attribute_name, target_name=\"Class\"):\n",
    "    # Calculate the entropy of the whole dataset\n",
    "    total_entropy = entropy(df[target_name])\n",
    "    \n",
    "    # Values and counts of the split attribute\n",
    "    vals, counts = np.unique(df[split_attribute_name], return_counts=True)\n",
    "    \n",
    "    # Weighted entropy after split\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * \n",
    "                           entropy(df.where(df[split_attribute_name] == vals[i]).dropna()[target_name])\n",
    "                           for i in range(len(vals)))\n",
    "    \n",
    "    # Information gain is the difference in entropy before and after the split\n",
    "    info_gain = total_entropy - weighted_entropy\n",
    "    return info_gain\n",
    "\n",
    "gain_attribute1 = information_gain(df, 'Attribute1')\n",
    "gain_attribute2 = information_gain(df, 'Attribute2')\n",
    "\n",
    "# For Attribute3 (continuous), we need to handle it differently\n",
    "# Here, we assume a binary split at a specific value, for simplicity, we'll use the median\n",
    "median_value = df['Attribute3'].median()\n",
    "df['Attribute3_split'] = df['Attribute3'] <= median_value\n",
    "gain_attribute3 = information_gain(df, 'Attribute3_split')\n",
    "\n",
    "# Print the information gains\n",
    "print(f\"Information Gain for Attribute 1: {gain_attribute1}\")\n",
    "print(f\"Information Gain for Attribute 2: {gain_attribute2}\")\n",
    "print(f\"Information Gain for Attribute 3 (split at median): {gain_attribute3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "170810ed-7d72-4918-8bfc-9b9eff7faf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split at 2.00 gives information gain of 0.14269\n",
      "Split at 3.50 gives information gain of 0.00257\n",
      "Split at 4.50 gives information gain of 0.07278\n",
      "Split at 5.50 gives information gain of 0.00721\n",
      "Split at 6.50 gives information gain of 0.01831\n",
      "Split at 7.50 gives information gain of 0.10219\n",
      "Best split point for Attribute 3 is at 2.0 with information gain of 0.14269027946047552\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using entropy as the measure of node impurity and the information gain metric, what is the best split point for Attribute 3?\n",
    "'''\n",
    "\n",
    "# Function to calculate information gain for a given split point\n",
    "def information_gain_continuous(df, split_attribute_name, split_value, target_name=\"Class\"):\n",
    "    # Calculate the entropy of the whole dataset\n",
    "    total_entropy = entropy(df[target_name])\n",
    "    \n",
    "    # Split the dataset\n",
    "    df_left = df[df[split_attribute_name] <= split_value]\n",
    "    df_right = df[df[split_attribute_name] > split_value]\n",
    "    \n",
    "    # Calculate the entropy for each subset\n",
    "    left_entropy = entropy(df_left[target_name])\n",
    "    right_entropy = entropy(df_right[target_name])\n",
    "    \n",
    "    # Calculate the weighted average entropy after split\n",
    "    weighted_entropy = (len(df_left) / len(df)) * left_entropy + (len(df_right) / len(df)) * right_entropy\n",
    "    \n",
    "    # Information gain is the difference in entropy before and after the split\n",
    "    info_gain = total_entropy - weighted_entropy\n",
    "    return info_gain\n",
    "    \n",
    "# Find the unique values in Attribute 3 and sort them\n",
    "unique_values = sorted(df['Attribute3'].unique())\n",
    "\n",
    "# Calculate the midpoints between consecutive unique values\n",
    "split_points = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values)-1)]\n",
    "\n",
    "# Evaluate information gain for each split point\n",
    "best_split = None\n",
    "best_info_gain = -1\n",
    "\n",
    "for split_point in split_points:\n",
    "    info_gain = information_gain_continuous(df, 'Attribute3', split_point)\n",
    "    print(f\"Split at {split_point:.2f} gives information gain of {info_gain:.5f}\")\n",
    "    if info_gain > best_info_gain:\n",
    "        best_info_gain = info_gain\n",
    "        best_split = split_point\n",
    "\n",
    "# Output the best split point\n",
    "print(f\"Best split point for Attribute 3 is at {best_split} with information gain of {best_info_gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a4bbaf1-4141-4a88-aedf-ea2b2c80f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification Error for Attribute 1: 0.2222222222222222\n",
      "Misclassification Error for Attribute 2: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using the misclassification error as the measure of node impurity, which attribute, between Attribute 1 and Attribute 2, provides the best split? What is the misclassification error rate for the split?\n",
    "\n",
    "The attribute with the lower misclassification error is the better attribute for splitting the data, as it results in fewer misclassified instances in the resulting nodes.\n",
    "'''\n",
    "data2 = {\n",
    "    'Attribute1': ['T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F'],\n",
    "    'Attribute2': ['T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T'],\n",
    "    'Attribute3': [1, 6, 5, 4, 7, 3, 8, 7, 5],\n",
    "    'Class': ['Y', 'Y', 'N', 'Y', 'N', 'N', 'N', 'Y', 'N']\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "def misclassification_error(y):\n",
    "    class_counts = y.value_counts()\n",
    "    max_class = class_counts.max()\n",
    "    error = 1 - max_class / len(y)\n",
    "    return error\n",
    "\n",
    "def misclassification_error_split(df, split_attribute_name, target_name=\"Class\"):\n",
    "    # Values of the split attribute\n",
    "    vals = df[split_attribute_name].unique()\n",
    "    \n",
    "    # Calculate the weighted misclassification error\n",
    "    weighted_error = 0\n",
    "    for val in vals:\n",
    "        subset = df[df[split_attribute_name] == val][target_name]\n",
    "        subset_error = misclassification_error(subset)\n",
    "        weighted_error += len(subset) / len(df) * subset_error\n",
    "    \n",
    "    return weighted_error\n",
    "\n",
    "# Calculate misclassification error for Attribute 1 and Attribute 2\n",
    "error_attribute1 = misclassification_error_split(df2, 'Attribute1')\n",
    "error_attribute2 = misclassification_error_split(df2, 'Attribute2')\n",
    "\n",
    "# Print the misclassification errors\n",
    "print(f\"Misclassification Error for Attribute 1: {error_attribute1}\")\n",
    "print(f\"Misclassification Error for Attribute 2: {error_attribute2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a66806-cd35-4017-a80b-a6ff5e911e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Index for Attribute 1: 0.34444444444444433\n",
      "Gini Index for Attribute 2: 0.4888888888888889\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Figure 1: Training Dataset\n",
    "Instance\n",
    "Attribute 1\n",
    "Attribute 2\n",
    "Attribute 3\n",
    "Class\n",
    "1\tT\tT\t1\tY\n",
    "2\tT\tT\t6\tY\n",
    "3\tT\tF\t5\tN\n",
    "4\tF\tF\t4\tY\n",
    "5\tF\tT\t7\tN\n",
    "6\tF\tT\t3\tN\n",
    "7\tF\tF\t8\tN\n",
    "8\tT\tF\t7\tY\n",
    "9\tF\tT\t5\tN\n",
    " \n",
    "\n",
    "Review the table labeled Figure 1: Training Dataset. Assume that we want to use a decision tree for modeling the data.\n",
    "\n",
    "Using the Gini Index as the measure of node impurity, which attribute, between Attribute 1 and Attribute 2, provides the best split? What is the Gini Index for the split?\n",
    "\n",
    "After running the code, you will get the Gini Index values for both attributes. The attribute with the lower Gini Index provides the best split.\n",
    "'''\n",
    "# Create the data\n",
    "data3 = {\n",
    "    'Attribute1': ['T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F'],\n",
    "    'Attribute2': ['T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T'],\n",
    "    'Attribute3': [1, 6, 5, 4, 7, 3, 8, 7, 5],\n",
    "    'Class': ['Y', 'Y', 'N', 'Y', 'N', 'N', 'N', 'Y', 'N']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "def gini_index(y):\n",
    "    class_counts = y.value_counts()\n",
    "    total_instances = len(y)\n",
    "    gini = 1.0\n",
    "    for count in class_counts:\n",
    "        gini -= (count / total_instances) ** 2\n",
    "    return gini\n",
    "\n",
    "def gini_index_split(df, split_attribute_name, target_name=\"Class\"):\n",
    "    # Values of the split attribute\n",
    "    vals = df[split_attribute_name].unique()\n",
    "    \n",
    "    # Calculate the weighted Gini index\n",
    "    weighted_gini = 0\n",
    "    for val in vals:\n",
    "        subset = df[df[split_attribute_name] == val][target_name]\n",
    "        subset_gini = gini_index(subset)\n",
    "        weighted_gini += len(subset) / len(df) * subset_gini\n",
    "    \n",
    "    return weighted_gini\n",
    "\n",
    "# Calculate Gini index for Attribute 1 and Attribute 2\n",
    "gini_attribute1 = gini_index_split(df3, 'Attribute1')\n",
    "gini_attribute2 = gini_index_split(df3, 'Attribute2')\n",
    "\n",
    "# Print the Gini Index results\n",
    "print(f\"Gini Index for Attribute 1: {gini_attribute1}\")\n",
    "print(f\"Gini Index for Attribute 2: {gini_attribute2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9552ba9-77d1-4926-9268-6ed5fcbb4087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Figure 2: Model Predictions for Images\n",
    "Ground Truth\n",
    "apple\tapple\tapple\tapple\tapple\tapple\tapple\tapple\tapple\tapple\n",
    "Predict\n",
    "apple\tpear\tpear\tapple\tapple\tpear\tpear\tpear\tapple\tapple\n",
    " \n",
    "\n",
    "Review the table labeled Figure 2: Model Predictions for Images. You trained a model to predict types of fruits in images. The model is tested against 10 images which only include apples in them. The model predictions are recorded in the table.\n",
    "\n",
    "What is the model precision and recall for ‘apple’ class?\n",
    "\n",
    "Group of answer choices\n",
    "\n",
    "Precision – 1 and recall – 1\n",
    "\n",
    "Precision – 0.5 and recall – 0.5\n",
    "\n",
    "Precision – 1 and recall – 0.5\n",
    "\n",
    "Precision – 0.5 and recall – 1\n",
    "'''\n",
    "# Ground truth and predictions\n",
    "ground_truth = ['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple']\n",
    "predictions = ['apple', 'pear', 'pear', 'apple', 'apple', 'pear', 'pear', 'pear', 'apple', 'apple']\n",
    "\n",
    "# Calculate TP, FP, and FN\n",
    "TP = sum((gt == 'apple') and (pred == 'apple') for gt, pred in zip(ground_truth, predictions))\n",
    "FP = sum((gt != 'apple') and (pred == 'apple') for gt, pred in zip(ground_truth, predictions))  # In this case, FP is 0 because all ground truth are apples\n",
    "FN = sum((gt == 'apple') and (pred != 'apple') for gt, pred in zip(ground_truth, predictions))\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc4675da-7cd4-45ca-aad2-eec07168a4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.67)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Figure 2: Model Predictions for Images\n",
    "Ground Truth\n",
    "apple\tapple\tapple\tapple\tapple\tapple\tapple\tapple\tapple\tapple\n",
    "Predict\n",
    "apple\tpear\tpear\tapple\tapple\tpear\tpear\tpear\tapple\tapple\n",
    " \n",
    "\n",
    "Review the table labeled Figure 2: Model Predictions for Images. You trained a model to predict types of fruits in images. The model is tested against 10 images which only include apples in them. The model predictions are recorded in the table.\n",
    "\n",
    "What is the model F1 score for the “apple” class?\n",
    "\n",
    "Your answer should be in format X.XX or X.X (depending on the precision required on decimal places).\n",
    "'''\n",
    "\n",
    "# Ground truth and predictions\n",
    "ground_truth = ['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple']\n",
    "predictions = ['apple', 'pear', 'pear', 'apple', 'apple', 'pear', 'pear', 'pear', 'apple', 'apple']\n",
    "\n",
    "# Calculate TP, FP, and FN\n",
    "TP = sum((gt == 'apple') and (pred == 'apple') for gt, pred in zip(ground_truth, predictions))\n",
    "FP = sum((gt != 'apple') and (pred == 'apple') for gt, pred in zip(ground_truth, predictions))  # In this case, FP is 0\n",
    "FN = sum((gt == 'apple') and (pred != 'apple') for gt, pred in zip(ground_truth, predictions))\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Output the F1 Score formatted to two decimal places\n",
    "f1_score_rounded = round(f1_score, 2)\n",
    "f1_score, f1_score_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbcf5106-41aa-4a5d-841a-8a09c7e3a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost for M1: 34000.0\n",
      "Total Cost for M2: 25000.0\n",
      "Choose M2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Consider the cost matrix shown above. Consider a machine M1 that out of 10,000 faulty parts classifies 7000 as faulty and 3000 as not faulty. Out of 10,000 not faulty parts classifies 4000 as faulty and 6000 as not faulty. Consider another machine M2, that out of the same 10,000 faulty parts classifies 8000 as faulty and 2000 as not faulty. Out of 10,000 not faulty parts classifies 5000 as faulty and 5000 as not faulty.\n",
    "\n",
    "Which machine will you choose?\n",
    "\n",
    "Predicted Faulty\n",
    "Predicted Not Faulty\n",
    "Faulty\n",
    "0\t10.0\n",
    "Not Faulty\n",
    "\n",
    "Understanding the Cost Matrix:\n",
    "Faulty Part classified as Faulty: Cost = 0\n",
    "Faulty Part classified as Not Faulty: Cost = 10.0\n",
    "Not Faulty Part classified as Faulty: Cost = 1.0\n",
    "Not Faulty Part classified as Not Faulty: Cost = 0\n",
    "\n",
    "Machine M1 Performance:\n",
    "Faulty parts: 7,000 correctly classified as Faulty, 3,000 incorrectly classified as Not Faulty.\n",
    "Not Faulty parts: 4,000 incorrectly classified as Faulty, 6,000 correctly classified as Not Faulty.\n",
    "'''\n",
    "\n",
    "# Define the number of parts for each scenario\n",
    "total_faulty_parts = 10000\n",
    "total_not_faulty_parts = 10000\n",
    "\n",
    "# Machine M1 performance\n",
    "# M1 classifies 7000 faulty parts correctly, 3000 faulty parts incorrectly\n",
    "M1_correct_faulty = 7000\n",
    "M1_incorrect_faulty = 3000\n",
    "\n",
    "# M1 classifies 6000 not faulty parts correctly, 4000 not faulty parts incorrectly\n",
    "M1_correct_not_faulty = 6000\n",
    "M1_incorrect_not_faulty = 4000\n",
    "\n",
    "# Machine M2 performance\n",
    "# M2 classifies 8000 faulty parts correctly, 2000 faulty parts incorrectly\n",
    "M2_correct_faulty = 8000\n",
    "M2_incorrect_faulty = 2000\n",
    "\n",
    "# M2 classifies 5000 not faulty parts correctly, 5000 not faulty parts incorrectly\n",
    "M2_correct_not_faulty = 5000\n",
    "M2_incorrect_not_faulty = 5000\n",
    "\n",
    "# Cost matrix\n",
    "# Cost of classifying a faulty part as not faulty\n",
    "cost_faulty_as_not_faulty = 10.0\n",
    "\n",
    "# Cost of classifying a not faulty part as faulty\n",
    "cost_not_faulty_as_faulty = 1.0\n",
    "\n",
    "# Calculate the total cost for M1\n",
    "cost_M1 = (M1_incorrect_faulty * cost_faulty_as_not_faulty) + (M1_incorrect_not_faulty * cost_not_faulty_as_faulty)\n",
    "\n",
    "# Calculate the total cost for M2\n",
    "cost_M2 = (M2_incorrect_faulty * cost_faulty_as_not_faulty) + (M2_incorrect_not_faulty * cost_not_faulty_as_faulty)\n",
    "\n",
    "# Print out the total costs for both machines\n",
    "print(f\"Total Cost for M1: {cost_M1}\")\n",
    "print(f\"Total Cost for M2: {cost_M2}\")\n",
    "\n",
    "# Determine which machine has the lower cost\n",
    "if cost_M1 < cost_M2:\n",
    "    print(\"Choose M1\")\n",
    "elif cost_M2 < cost_M1:\n",
    "    print(\"Choose M2\")\n",
    "else:\n",
    "    print(\"Either machine can be chosen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6740b9c0-7562-4b8d-b133-aa3546f3c5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Entropy: 0.9910760598382222\n",
      "Information Gain for Attribute 1: 0.22943684069673975\n",
      "Information Gain for Attribute 2: 0.007214618474517431\n",
      "Information Gain for Attribute 3: 0.14269027946047552 (Threshold: 2.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    'Attribute 1': ['T', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F'],\n",
    "    'Attribute 2': ['T', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T'],\n",
    "    'Attribute 3': [1, 6, 5, 4, 7, 3, 8, 7, 5],\n",
    "    'Class': ['Y', 'Y', 'N', 'Y', 'N', 'N', 'N', 'Y', 'N']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(probs):\n",
    "    return -sum([p * np.log2(p) for p in probs if p > 0])\n",
    "\n",
    "# Function to calculate entropy of the dataset\n",
    "def entropy_of_dataset(df, target_column):\n",
    "    target_values = df[target_column].value_counts(normalize=True)\n",
    "    return entropy(target_values)\n",
    "\n",
    "# Function to calculate information gain\n",
    "def information_gain(df, split_attribute, target_attribute='Class'):\n",
    "    total_entropy = entropy_of_dataset(df, target_attribute)\n",
    "    \n",
    "    # Get the unique values of the attribute we are splitting by\n",
    "    values = df[split_attribute].unique()\n",
    "    \n",
    "    # Calculate the weighted entropy after the split\n",
    "    weighted_entropy = 0\n",
    "    for value in values:\n",
    "        subset = df[df[split_attribute] == value]\n",
    "        subset_entropy = entropy_of_dataset(subset, target_attribute)\n",
    "        weighted_entropy += (len(subset) / len(df)) * subset_entropy\n",
    "    \n",
    "    # Information gain is the reduction in entropy\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Entropy before any split\n",
    "initial_entropy = entropy_of_dataset(df, 'Class')\n",
    "print(f\"Initial Entropy: {initial_entropy}\")\n",
    "\n",
    "# Calculate information gain for Attribute 1 and Attribute 2\n",
    "gain_attribute_1 = information_gain(df, 'Attribute 1')\n",
    "gain_attribute_2 = information_gain(df, 'Attribute 2')\n",
    "\n",
    "print(f\"Information Gain for Attribute 1: {gain_attribute_1}\")\n",
    "print(f\"Information Gain for Attribute 2: {gain_attribute_2}\")\n",
    "\n",
    "# To handle Attribute 3 (continuous), we need to find the best threshold for splitting.\n",
    "# Let's calculate the gain for each possible threshold:\n",
    "def information_gain_continuous(df, attribute, target_attribute='Class'):\n",
    "    # Sort by the continuous attribute\n",
    "    df_sorted = df.sort_values(attribute)\n",
    "    values = df_sorted[attribute].unique()\n",
    "    \n",
    "    best_gain = -1\n",
    "    best_threshold = None\n",
    "    \n",
    "    # Try every possible split point (midpoints between adjacent values)\n",
    "    for i in range(len(values) - 1):\n",
    "        threshold = (values[i] + values[i + 1]) / 2\n",
    "        \n",
    "        # Create two subsets: less than and greater than or equal to the threshold\n",
    "        df_left = df[df[attribute] <= threshold]\n",
    "        df_right = df[df[attribute] > threshold]\n",
    "        \n",
    "        # Calculate weighted entropy\n",
    "        left_entropy = entropy_of_dataset(df_left, target_attribute)\n",
    "        right_entropy = entropy_of_dataset(df_right, target_attribute)\n",
    "        weighted_entropy = (len(df_left) / len(df)) * left_entropy + (len(df_right) / len(df)) * right_entropy\n",
    "        \n",
    "        # Information gain for this split\n",
    "        gain = initial_entropy - weighted_entropy\n",
    "        \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_gain, best_threshold\n",
    "\n",
    "gain_attribute_3, threshold_3 = information_gain_continuous(df, 'Attribute 3')\n",
    "print(f\"Information Gain for Attribute 3: {gain_attribute_3} (Threshold: {threshold_3})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c75b19-e87f-4f72-87d8-6c49ce26600e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
